import nltk
import random
import string
import json
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# üîπ D√©finir un r√©pertoire sp√©cifique pour NLTK
nltk_data_path = os.path.join(os.path.dirname(__file__), 'nltk_data')
if not os.path.exists(nltk_data_path):
    os.makedirs(nltk_data_path)

# Sp√©cifier √† NLTK d'utiliser ce r√©pertoire
nltk.data.path.append(nltk_data_path)

# V√©rifie si 'punkt' est t√©l√©charg√©, sinon le t√©l√©charger dans le r√©pertoire personnalis√©
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("T√©l√©chargement du tokenizer 'punkt'...")
    nltk.download('punkt', download_dir=nltk_data_path)

# T√©l√©charge 'stopwords' si n√©cessaire
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    print("T√©l√©chargement des 'stopwords'...")
    nltk.download('stopwords', download_dir=nltk_data_path)

# Afficher les chemins de recherche de NLTK pour confirmer o√π il cherche les donn√©es
print("Chemins de recherche des donn√©es NLTK:", nltk.data.path)

class Chatbot:
    def __init__(self, response_file='chatbot_responses.json'):
        # üìÇ Obtenir le chemin absolu vers le fichier JSON
        current_dir = os.path.dirname(__file__)
        response_path = os.path.join(current_dir, response_file)

        # üìÑ Charger les r√©ponses depuis le fichier JSON
        with open(response_path, 'r', encoding='utf-8') as f:
            self.responses = json.load(f)

        # ‚öôÔ∏è Pr√©parer les questions pour la vectorisation TF-IDF
        self.vectorizer = TfidfVectorizer()
        self.questions = list(self.responses.keys())
        self.vectorized_questions = self.vectorizer.fit_transform(self.questions)

    def get_response(self, user_input):
        # üî° Normaliser l'entr√©e utilisateur (minuscule, sans ponctuation)
        user_input = user_input.lower()
        user_input = ''.join([c for c in user_input if c not in string.punctuation])

        # üß† Essayer de tokeniser, et re-t√©l√©charger punkt si besoin
        try:
            tokens = nltk.word_tokenize(user_input)
        except LookupError:
            print("Erreur de tokenisation, t√©l√©chargement de 'punkt'...")
            nltk.download('punkt', download_dir=nltk_data_path)
            tokens = nltk.word_tokenize(user_input)

        if not tokens:
            return "Je n'ai pas compris. Peux-tu reformuler ?"

        # üìä Vectoriser la question utilisateur
        user_vector = self.vectorizer.transform([user_input])
        similarities = cosine_similarity(user_vector, self.vectorized_questions)

        # üîç Trouver la meilleure correspondance
        best_match = similarities.argmax()
        score = similarities[0, best_match]

        if score > 0.3:
            return random.choice(self.responses[self.questions[best_match]])
        else:
            return random.choice([
                "D√©sol√©, je ne comprends pas.",
                "Peux-tu reformuler ?",
                "Je ne sais pas r√©pondre √† √ßa."
            ])
